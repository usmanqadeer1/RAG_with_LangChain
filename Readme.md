# ğŸ§  LLM Chatbot with RAG (Retrieval-Augmented Generation)

A full-stack, streaming AI chatbot application built with:

- ğŸ” **LangChain** for RAG logic  
- ğŸ¦™ **LLaMA 3 via Ollama** as the local language model  
- âš¡ **FastAPI** for the backend with SSE streaming  
- ğŸ’¬ **React** for the frontend chat interface  

---

## ğŸš€ Features

- ğŸ” **Streaming responses** in real-time using Server-Sent Events (SSE)  
- ğŸ§  **Chat history aware** (context from previous turns is used)  
- ğŸ¨ Clean and responsive frontend UI (React)  
- ğŸŒ CORS-enabled backend for local frontend/backend dev  

---

## ğŸ“ Project Structure

rag-with-langchain/
â”‚
â”œâ”€â”€ backend/
â”‚ â”œâ”€â”€ ingest.py # Loads and chunks documents, creates FAISS index
â”‚ â”œâ”€â”€ rag_chain.py # Defines RAG chain using LangChain and Ollama
â”‚ â”œâ”€â”€ app.py # FastAPI app exposing a /chat streaming endpoint
â”‚ â””â”€â”€ docs/ # Optional: Folder to store any local files
â”‚
â”œâ”€â”€ frontend/
â”‚ â”œâ”€â”€ public/
â”‚ â”œâ”€â”€ src/
â”‚ â”‚ â”œâ”€â”€ Chat.js # React chat component with streaming logic
â”‚ â”‚ â””â”€â”€ index.js # App entry point
â”‚ â””â”€â”€ package.json # React app config
â”‚
â”œâ”€â”€ requirements.txt # Python dependencies
â””â”€â”€ README.md 


---

## ğŸ”§ Installation

### 1. Backend Setup

Make sure you have Python 3.10+ and [Ollama](https://ollama.com) installed.

```bash
cd backend
python -m venv venv
source venv/bin/activate      # On Windows: venv\Scripts\activate
pip install -r requirements.txt
uvicorn app:app --reload

cd rag-bot
npm install
npm start
