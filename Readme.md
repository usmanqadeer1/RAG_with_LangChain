# ğŸ§  LLM Chatbot with RAG (Retrieval-Augmented Generation)

A full-stack, streaming AI chatbot application built with:

- ğŸ” **LangChain** for RAG logic  
- ğŸ¦™ **LLaMA 3 via Ollama** as the local language model  
- âš¡ **FastAPI** for the backend with SSE streaming  
- ğŸ’¬ **React** for the frontend chat interface  

---

## ğŸš€ Features

- ğŸ” **Streaming responses** in real-time using Server-Sent Events (SSE)  
- ğŸ§  **Chat history aware** (context from previous turns is used)  
- ğŸ¨ Clean and responsive frontend UI (React)  
- ğŸŒ CORS-enabled backend for local frontend/backend dev  

---

## ğŸ”§ Installation

### 1. Backend Setup

Make sure you have Python 3.10+ and [Ollama](https://ollama.com) installed.

```bash
cd backend
python -m venv venv
source venv/bin/activate      # On Windows: venv\Scripts\activate
pip install -r requirements.txt
uvicorn app:app --reload

cd rag-bot
npm install
npm start
